{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb87a679",
   "metadata": {},
   "source": [
    "# May 22-26 Week\n",
    "\n",
    "#### James LeÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad909c5",
   "metadata": {},
   "source": [
    "## Advances on the Udacity Summaries \n",
    "- The course was completed up to 60% from the previous 20%\n",
    "To check the notes please refer to the following notion which will be soon uploaded to github as a PDF for more convenient access: \n",
    "\n",
    "[Notes on Udacity](https://jamesxleon.notion.site/AI-for-Trading-94dd5c1df1834c4e97c254571aac6373)\n",
    "\n",
    "#### Please refer to the next section of the report for Ney's text summarization using the OpenAI API\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd9f19",
   "metadata": {},
   "source": [
    "## Chat GPT app to summarize text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23081a3",
   "metadata": {},
   "source": [
    "### Brief\n",
    "\n",
    "##### [138 conversations were summarized with this approach](summaries.txt)\n",
    "\n",
    "A tutorial was created for this project which is attached after the code. The complete code of the project can be found on the [github repository]() \n",
    "\n",
    "Here an explanation on the use:\n",
    "\n",
    "1. The SimpleTokenizer.py receives a text file from accessapi.py and returns a processed result. This process is done once and creates the parsed_data.pkl structure to store the resulting objects to avoid repeating unnecesary steps.\n",
    "2. When the parsed_data.pkl file doesn't exist, the SimpleTokenizer.py parses the received text to a dictionary made of a title and chunks of text smaller than 1700 words (To avoid surpassing the GPT models token limit).\n",
    "3. Accessapi.py also stores a control value in the last_processed.txt which helps to re run the script in case of failure.\n",
    "4. The accessapi.py connects to the OpenAI server and prompts the instructions with the chunks of text corresponding to a title from the generate_summary function.\n",
    "5. The summarize_streams() joins all summarized chunks together to create a single summary for each title. \n",
    "6. Resulting summaries are written to the summaries.txt file. Every time the script needs to be re run due to an error, the new summaries are appended to previous ones.\n",
    "\n",
    "##### Caution: Some results may have inconsistent answers or typos. However, trial and error demonstrated that the best values to configure the model for this task are as set in accesapi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e6a39",
   "metadata": {},
   "source": [
    "### The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleTokenizer\n",
    "\n",
    "import re\n",
    "\n",
    "class Stream:\n",
    "    def __init__(self, title):\n",
    "        self.title = title\n",
    "        self.parts = []\n",
    "        self.current_part_words = 0\n",
    "        self.current_part = []\n",
    "\n",
    "    def add_words(self, words):\n",
    "        for word in words:\n",
    "            if self.current_part_words + len(word.split()) > 1700:\n",
    "                self.parts.append(' '.join(self.current_part))\n",
    "                self.current_part = [word]\n",
    "                self.current_part_words = len(word.split())\n",
    "            else:\n",
    "                self.current_part.append(word)\n",
    "                self.current_part_words += len(word.split())\n",
    "\n",
    "    def finalize(self):\n",
    "        if self.current_part:\n",
    "            self.parts.append(' '.join(self.current_part))\n",
    "\n",
    "def parse_file(filename):\n",
    "    with open(filename, 'r', encoding='cp1252') as file: #Here you should choose the right encoding for your text, this one worked for me \n",
    "        content = file.readlines()\n",
    "\n",
    "    streams = []\n",
    "    current_stream = None\n",
    "\n",
    "    for line in content:\n",
    "        line = line.strip()  # remove leading/trailing whitespace\n",
    "        if line.startswith(\"Stream name:\"):\n",
    "            if current_stream is not None:\n",
    "                current_stream.finalize()\n",
    "                streams.append(current_stream)\n",
    "                #counter += 1\n",
    "            current_stream = Stream(line[len(\"Stream name:\"):].strip())\n",
    "        elif current_stream is not None and line != '':  # ignore blank lines\n",
    "            current_stream.add_words(re.split(r'\\s+', line))\n",
    "\n",
    "    if current_stream is not None:\n",
    "        current_stream.finalize()\n",
    "        streams.append(current_stream)\n",
    "\n",
    "    return [{'title': stream.title, 'parts': stream.parts} for stream in streams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99766935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access Api\n",
    "import openai\n",
    "import os\n",
    "import pickle\n",
    "from alive_progress import alive_bar\n",
    "from local_settings import secret_key\n",
    "from SimpleTokenizer import parse_file\n",
    "\n",
    "openai.api_key = secret_key\n",
    "\n",
    "def generate_summary(text):\n",
    "    # Ensure text length is within GPT-3's maximum input size\n",
    "    if len(text) > 2048:\n",
    "        text = text[:2048]\n",
    "    \n",
    "    # Add specific instructions to the prompt\n",
    "    prompt = f\"The following text is a conversation from a trading transcript: \\\"{text}\\\". Summarize the most important trading-related information in four concise bullet points. After the bullet points, provide a brief paragraph suggesting potential actions and key ideas based on the summarized points, all in the style of Warren Buffet. Always penalize redundancy. Write only sentences with full ideas\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.4,\n",
    "        max_tokens=175\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def summarize_stream(Sumstream):\n",
    "    summarized_parts = [generate_summary(part) for part in Sumstream['parts']]\n",
    "    return ' '.join(summarized_parts)\n",
    "\n",
    "filename = 'transcripts.txt'  # or wherever your file is located\n",
    "\n",
    "# Check if parsed data exists\n",
    "if os.path.exists('parsed_data.pkl'):\n",
    "    with open('parsed_data.pkl', 'rb') as f:\n",
    "        parsed_data = pickle.load(f)\n",
    "else:\n",
    "    parsed_data = parse_file(filename)\n",
    "    # Save parsed data\n",
    "    with open('parsed_data.pkl', 'wb') as f:\n",
    "        pickle.dump(parsed_data, f)\n",
    "\n",
    "# Open a file for writing\n",
    "with open('summaries.txt', 'a', encoding='UTF-8') as f:\n",
    "    \n",
    "    last_processed_title = \"\"\n",
    "    if os.path.exists('last_processed.txt'):\n",
    "        with open('last_processed.txt', 'r', encoding='UTF-8') as lp:\n",
    "            last_processed_title = lp.read().strip()\n",
    "\n",
    "    start_processing = False if last_processed_title else True\n",
    "\n",
    "    with alive_bar(len(parsed_data)) as bar:\n",
    "        for stream in parsed_data:\n",
    "            if not start_processing:\n",
    "                if stream['title'] == last_processed_title:\n",
    "                    start_processing = True\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open('last_processed.txt', 'w', encoding='UTF-8') as lp:\n",
    "                    lp.write(stream['title'])\n",
    "                summary = summarize_stream(stream)\n",
    "                f.write(f\"Title: {stream['title']}\\n\")\n",
    "                f.write(f\"Summary: {summary}\\n\\n\")\n",
    "                bar()\n",
    "            except Exception as e:\n",
    "                print(f\"Error while processing title: {stream['title']}\")\n",
    "                print(str(e))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee9e94",
   "metadata": {},
   "source": [
    "### The Results\n",
    "\n",
    "Let's take the first conversation from the script â€”which turned out to be a really long one.\n",
    "\n",
    "```Stream name: $1.3M gain on $TVIX.```\n",
    "\n",
    ">Morning, guys.\n",
    "Can you hear me? Okay, great, great, great.\n",
    "Yes, you bet I did.\n",
    "You bet I did.\n",
    "Oh, yeah.\n",
    "I caught some tweak short yesterday.\n",
    "Oh, yeah.\n",
    "Oh, yeah.\n",
    "I did.\n",
    "Oh, it's some big size.\n",
    "Oh, yeah.\n",
    "I'm going to talk about it later.\n",
    "Oh, you bet I caught.\n",
    "Oh, yes.\n",
    "Oh, yes, I did.\n",
    "I did.\n",
    "Big time.\n",
    "Big time.\n",
    "Oh, yeah.\n",
    "I got some tweaks short yesterday.\n",
    "Yes, I did.\n",
    "I was close to that.\n",
    "Yes, I was close to that before it bounds, yes, at 1.3 I was up 1.51 0.6 at 1 point But then it had a bounce later I held some overnight and then it cap down and now it's kind of like grinding higher in premarket, I'm going to say.\n",
    "Yes.\n",
    "It was about 1.3 -- well, it was 1.3 when I took in Charlotte I was up at 1.51 0.1 Yes.\n",
    "Oh, yes.\n",
    "Oh, yes.\n",
    "And I could see the decoupling.\n",
    "You could clearly see when it started decoupling from the indices.\n",
    "Like here, this is where I will short some like before, but here is when I started really, really adding because I saw like here 12:45, like the Spice, they were nowhere near their highs or 12 5 here is well 45 They were nowhere near its highs, but TVIX started taking out the lows of the day or was very close to lows of the day.\n",
    "And here, when it built these lower highs, I added some here.\n",
    "Here too, where is it? 145 145, where is it here? Like the Spice, again, Spice, we're building lower highs on the 5 minute chart, and they were nowhere near at the highs of the day, and Twicks was also building lower highs, and that had a range break This being multi day range break, that's where I added some size.\n",
    "It went down over 30 percent in like an hour or 2 That was just if you clearly see the relative weakness on Twigg.\n",
    "\n",
    "(4920 words laterâ€¦)\n",
    "\n",
    ">I am not on a radar, there's a lot today, probably mostly going to focus on TVA.\n",
    "It looks like it's about to take out loads of the day again.\n",
    "I don't know if I'm going at this time or not, because now I have pretty decent size.\n",
    "And I don't want to get my average too low, but you never know.\n",
    "I think like it looks like the size should -- the size have a big grip is like there's an air pocket here from mid-200s to like a 02:47 to 02:56 or something like a 02:57 big air pump Okay Yes.\n",
    "Thanks, guys.\n",
    "I hope you had a great trading week.\n",
    "And if not, I hope you have a great trading week next week.\n",
    "And have a great weekend.\n",
    "Thanks all to you on Monday..\n",
    "\n",
    "\n",
    "The total of words is 5395. Which may result in 4 chunks of text for the first title. Now, the prompt used for every summary is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48237b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"The following text is a conversation from a trading transcript: \\\"{text}\\\". Summarize the most important trading-related information in four concise bullet points. After the bullet points, provide a brief paragraph suggesting potential actions and key ideas based on the summarized points, all in the style of Warren Buffet. Always penalize redundancy. Write only sentences with full ideas\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8f577",
   "metadata": {},
   "source": [
    "Then, the results will be written to the .txt as 4 bullet points followed by a conclusion for every chunk of processed text:\n",
    "\n",
    ">Title: $1.3M gain on $TVIX.\n",
    ">\n",
    "- The trader shorted Twick yesterday and held some overnight.\n",
    "- The trader noticed Twick decoupling from the indices and built lower highs on the 5 minute chart.\n",
    "- The trader added size when Twick broke its multi-day range and went down over 30% in an hour or two.\n",
    "- The trader covered some of the position in the late fee market.\n",
    ">\n",
    ">Based on the summarized points, it is clear that the trader was able to identify a potential opportunity in Twick and capitalize on it. The trader was able to recognize the relative weakness in Twick and take advantage of the multi-day range break. This suggests that traders should always be on the lookout for potential opportunities and be willing to take calculated risks in order to maximize their profits. Warren Buffet's famous quote, \"Risk comes .\n",
    ">\n",
    "- Walter experienced a million dollar loss over the weekend.\n",
    "- The VIX was showing relative strength to the size.\n",
    "- Walter noticed a change in the VIX the day before, which was the first time in two weeks.\n",
    "- Walter shorted and added to his position as the VIX continued to decline.\n",
    ">\n",
    ">Based on the summarized points, it is clear that Walter was able to take advantage of a changing market to make a profit. He was able to identify a potential opportunity and act on it quickly. This is a key idea that Warren Buffet has always been known for; being able to recognize a good opportunity and taking action quickly. By doing this, Walter was able to make a profit despite the losses he experienced over the weekend. This is a great example of how being aware of the market and taking advantage of changes can be .\n",
    ">\n",
    "- Veeva is struggling and may present a big short opportunity.\n",
    "- Tilray, TVI, and BLPH are all stocks to watch.\n",
    "- Many stocks may have 50-200% bounce potential.\n",
    "- PLPH is building higher lows, so it may be a good idea to have a tight automated stop.\n",
    ">\n",
    ">Based on the summarized points, it appears that there are some stocks that may be worth watching, particularly Veeva and Tilray, which may present a big short opportunity. Additionally, there are many stocks that may have a 50-200% bounce potential, so it may be a good idea to keep an eye on those as well. Finally, PLPH is building higher lows, so it may be a good idea to have a tight automated stop in order to protect against any potential losses. Warren Buffet would .\n",
    ">\n",
    "- BLPH is intriguing and has the potential to go to 50% from the lows. \n",
    "- The speaker is considering shorting it instead of buying it. \n",
    "- The speaker is focusing on TVA and has a decent size position. \n",
    "- The speaker is not looking to lower their average price.\n",
    ">\n",
    ">Based on the summarized points, it appears that the speaker is looking to make a profit off of BLPH by shorting it instead of buying it. This is a risky move, but one that could potentially pay off if the stock goes to 50% from the lows. The speaker is also focusing on TVA, which appears to have a big grip and could be a good opportunity for a profit. The speaker also has a decent size position and is not looking to lower their average price. Overall, the speaker appears to be taking\n",
    "\n",
    "*Just as expected* the resulting text supposes four chunks of text resulting from the first conversation.\n",
    "\n",
    "*However*, the conclusions provided are mostly incomplete. The solutions to these are discussed in the following _What's Next_ section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627bb05",
   "metadata": {},
   "source": [
    "### What's Next \n",
    "\n",
    "#### Update the alive_bar \n",
    "\n",
    "`The alive_bar is always taking the whole dataset for its progress indications instead of the remaining values. This is a problem as it difficults to know the true remaining conversations to be processed`\n",
    "\n",
    "![The alive_bar!](img/TerminalSession.png)\n",
    "\n",
    "#### Get a paid API_key\n",
    "\n",
    "`Though the Models are not expensive to use ($0.02 per token~word). This code was created using the free $5 provided by OpenAI. That's the reason why the last error is not because of a connection interruption but an out of quota error.`\n",
    "\n",
    "#### Reformat the results\n",
    "\n",
    "`The results are being saved in the same format as they were received. Thus, for better visualization and comprehension, improvements can be made on the output`\n",
    "\n",
    "### The tutorial\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "- Python 3.9.16\n",
    "- pip install openai\n",
    "- optional: npm install openai\n",
    "- a valid OpenAI API key\n",
    "- other libraries\n",
    "\n",
    "#### Notes on this version of the code\n",
    "\n",
    ">The present version of the code won't need more help than the provided to this point. The present tutorial has been made for Ney use on how to create a full app using the OpenAI API. \n",
    "\n",
    "# Roadmap\n",
    "\n",
    "- Crash course on OpenAI and ChatGPT â†’ Environment setup â†’ Use the serverless API â†’ Integrate with platforms â†’ Create full real solution apps\n",
    "\n",
    "# Tech Stack of a ChatGPT app\n",
    "\n",
    "- Front-end (Like .bubble): The visuals\n",
    "- Back-end (Like .bubble): The code\n",
    "- Wrapper API (Azure funcs): Connection to Open AI\n",
    "- OpenAI API (API): Communication with the server\n",
    "- GPT-X Model (API): Response to the prompt, then goes all the way up again.\n",
    "\n",
    "From the wrapper down it will be always the same, but the back and the front depend on the environment. \n",
    "\n",
    "# Crash Course on OpenAI and ChatGPT\n",
    "\n",
    "### Using ChatGPT to answer problems\n",
    "\n",
    "- The basic structure of an interaction with ChatGPT is prompt â†’ response and back and forth.\n",
    "- The prompt: Needs an instruction, tone (politely, to a certain person on a certain situation, etc), context, detail, etc.\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "- Research and deployment company that manage models using APIs that you can deploy and integrate in your application (for a very cheap value to cost proportion).\n",
    "\n",
    "### OpenAI Models\n",
    "\n",
    "- GPT: Natural Language tasks with prompt\n",
    "    - DaVinci-003: The most capable and updated model. The highest quality with 4000 token size. The relative cost versus other models is high. Use it for everything.\n",
    "    - Curie-001: Great for nuanced tasks like sentiment analysis, language translation, and complex classification (i.e. right vs wrong equations). Use it for specific tasks.\n",
    "    - Babbage-001: For straightforward tasks like simple classification and semantic searching.\n",
    "    - Ada-001: For simple tasks like parsing text, address correction, and certain classification tasks.\n",
    "    - The takeaway on selecting a model: Use DaVinci for most business solutions.\n",
    "- Codex: Natural Language to code\n",
    "- DALL-E: Create and edit original images\n",
    "\n",
    "### DaVinci Capabilities\n",
    "\n",
    "- Classification: Statements into categories\n",
    "- Generation: Create something from an instruction\n",
    "- Conversation: Keep the context to follow up\n",
    "- Transformation: From a text\n",
    "- Completion: Fill the blanks\n",
    "\n",
    "### Prompt Optimization\n",
    "\n",
    "- Concepts\n",
    "    \n",
    "    **Prompt**: What you give the model || **Response**: What you get back from the model\n",
    "    \n",
    "    - Models are heavily depended on reacting to good prompts: Garbage in = Garbage out\n",
    "    \n",
    "    Tokens: the currency of ChatGPT: The model process text by breaking it down into smaller units called tokens\n",
    "    \n",
    "    <aside>\n",
    "    ðŸ“– 1 token ~= 4 characters\n",
    "    \n",
    "    75 words ~= 100 tokens \n",
    "    \n",
    "    **Total Tokens = prompt text + completed text**\n",
    "    \n",
    "    Max for DaVinci = 4000 tokens ~= 3000 words\n",
    "    \n",
    "    $0.02 per 1k tokens\n",
    "    \n",
    "    [Free trial of $18 ~= 900k tokens ~= 675k words] for developing and testing\n",
    "    \n",
    "    </aside>\n",
    "    \n",
    "    Temperature: From 0 to 1, sets the risk that the model uses for the response. 1 means more creative, 0 means a well-defined answer. [For a business solution you should go for 0]\n",
    "    \n",
    "- Be very instructional in what you want, and as specific as you want\n",
    "- Provide examples with good quality data:  Show and tell the model how to respond\n",
    "- Donâ€™t rely on factual responses\n",
    "\n",
    "# Environment Setup\n",
    "\n",
    "- **Get the API from**: [API keys - OpenAI API](https://platform.openai.com/account/api-keys)\n",
    "- **Get an Azure account from**: [# Shorten the text to fit into one assistant response\n",
    "\"Create Your Azure Free Account Today | Microsoft Azure](https://azure.microsoft.com/en-us/free/)\n",
    "    - As a recommendation, for developing and personal use â€˜Pay as you goâ€™ is the right match. However, some organizations and universities provide a free $200 credit for Microsoft Azure services.\n",
    "- On the Microsoft Azure workspace, search for Function App on the search bar â†’ Create. For categorization purposes, create a resource group named â€˜OpenAITestingâ€™.\n",
    "    \n",
    "    ![Screenshot 2023-05-25 at 18.04.41.png](img/AzureFunc1.png)\n",
    "    \n",
    "- On Networking make sure it is set to public. Everything else can be left as it is. However, you can connect the service to a github account for deployment.\n",
    "- Finally, go to Review+create and create.\n",
    "    \n",
    "    ![Screenshot 2023-05-25 at 18.07.30.png](img/AzureFunc2.png)\n",
    "    \n",
    "- The azure function would ping the OpenAI server.\n",
    "- **Make sure Python is installed and updated to 3.9.16 on your PC or virtual env (pyenv does the work, other versions of python canâ€™t handle Azure functions).**\n",
    "- Install VSCode and set a folder for the work environment.\n",
    "    - Check the interpreter is 3.9.16\n",
    "    - Install the azure functions extension (ms-azuretools.vscode-azurefunctions)\n",
    "    - Sign In to azure with the same account as before\n",
    "- For connection with [power apps](https://make.powerapps.com/environments/Default-9f119962-8c62-431c-a8ef-e7e0a42d11fc/home) and [power automate](https://make.powerautomate.com/environments/Default-9f119962-8c62-431c-a8ef-e7e0a42d11fc/home), connect your microsoft 365 account. (Using a temporary email you can access to the free trials on most of these).\n",
    "- All set!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e2e820688bba9d732968053fbb9c66ca14a5632f3a31f106d36db265d173bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
